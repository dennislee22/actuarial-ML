{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09bbc121-e8e3-4f28-a0c8-63d17652b4e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping addon with invalid or excluded ID: {'type': 'cmladdon', 'path': '/runtime-addons/cmladdon-2.0.49-b279', 'spec': '\\nenv:\\n  MLFLOW_TRACKING_URI: cml://localhost\\n  MLFLOW_REGISTRY_URI: cml://localhost\\n  PYTHONPATH: ${PYTHONPATH}:/opt/cmladdons/python/site-customize\\n  R_LIBS_SITE: ${R_LIBS_SITE}:/opt/cmladdons/r/libs\\npaths:\\n  - /opt/cmladdons', 'version': '', 'id': -1}\n",
      "Skipping addon with invalid or excluded ID: {'type': 'cmladdon', 'path': '/runtime-addons/cmladdon-2.0.49-b279', 'spec': '\\nenv:\\n  MLFLOW_TRACKING_URI: cml://localhost\\n  MLFLOW_REGISTRY_URI: cml://localhost\\n  PYTHONPATH: ${PYTHONPATH}:/opt/cmladdons/python/site-customize\\n  R_LIBS_SITE: ${R_LIBS_SITE}:/opt/cmladdons/r/libs\\npaths:\\n  - /opt/cmladdons', 'version': '', 'id': -1}\n",
      "\n",
      "Dask Diagnostic dashboard:\n",
      "https://qnsesx5st7vpkzjv.cmlws5.apps.dlee5.cldr.example/\n"
     ]
    }
   ],
   "source": [
    "import dask\n",
    "import time\n",
    "\n",
    "import cml.workers_v1 as workers\n",
    "dask_scheduler = workers.launch_workers(\n",
    "    n=1,\n",
    "    cpu=2,\n",
    "    memory=8,\n",
    "    code=f\"!dask-scheduler --host 0.0.0.0 --dashboard-address 127.0.0.1:8090\",\n",
    ")\n",
    "\n",
    "# Wait for the scheduler to start.\n",
    "time.sleep(10)\n",
    "\n",
    "scheduler_workers = workers.list_workers()\n",
    "scheduler_id = dask_scheduler[0][\"id\"]\n",
    "scheduler_ip = [\n",
    "    worker[\"ip_address\"] for worker in scheduler_workers if worker[\"id\"] == scheduler_id\n",
    "][0]\n",
    "\n",
    "scheduler_url = f\"tcp://{scheduler_ip}:8786\"\n",
    "\n",
    "k8s_pods = 5\n",
    "dask_workers = workers.launch_workers(\n",
    "    n=k8s_pods,\n",
    "    cpu=1,\n",
    "    memory=8,\n",
    "    code=f\"!dask-worker {scheduler_url}\",\n",
    ")\n",
    "\n",
    "# Wait for the workers to start.\n",
    "time.sleep(10)\n",
    "\n",
    "print(\"\\nDask Diagnostic dashboard:\")\n",
    "print(\"//\".join(dask_scheduler[0][\"app_url\"].split(\"//\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4bbc7d8d-8df5-4011-a285-87bb1a14687f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Dask-based Underwriting Model Training Process ---\n",
      "Dask client created. Dashboard at: <Client: 'tcp://10.42.1.124:8786' processes=5 threads=160, memory=36.78 GiB>\n",
      "Loading data from 'underwriting_data.csv' with Dask...\n",
      "Performing Dask feature engineering (One-Hot Encoding)...\n",
      "Data split into training and testing sets.\n",
      "\n",
      "Training Dask-XGBoost Regressor model for premium quotation...\n",
      "Model training complete.\n",
      "\n",
      "Model and column info bundled and saved to 'underwriting_bundle.joblib'.\n",
      "\n",
      "--- Evaluating Model Performance on Test Set ---\n",
      "R-squared (R²): 0.9963\n",
      "Mean Absolute Error (MAE): $126.23\n"
     ]
    }
   ],
   "source": [
    "import dask.dataframe as dd\n",
    "from dask.distributed import Client\n",
    "from dask_ml.model_selection import train_test_split\n",
    "from xgboost.dask import DaskXGBRegressor\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "import joblib\n",
    "import pandas as pd\n",
    "\n",
    "def train_underwriting_model_dask(data_file='underwriting_data.csv', model_bundle_output_file='underwriting_bundle.joblib'):\n",
    "    \"\"\"\n",
    "    Loads large-scale underwriting data using Dask, trains an XGBoost model, \n",
    "    and saves the model and its required columns into a single file.\n",
    "    \"\"\"\n",
    "    print(\"--- Starting Dask-based Underwriting Model Training Process ---\")\n",
    "    \n",
    "    # --- Set up a Dask Client ---\n",
    "    # This creates a local cluster on your machine to handle the parallel processing.\n",
    "    client = Client(scheduler_url)\n",
    "    print(f\"Dask client created. Dashboard at: {client}\")\n",
    "    \n",
    "    # --- Load Data with Dask ---\n",
    "    # Dask reads the data in partitions instead of loading it all into memory.\n",
    "    # blocksize specifies how large each chunk should be.\n",
    "    print(f\"Loading data from '{data_file}' with Dask...\")\n",
    "    ddf = dd.read_csv(data_file, blocksize=\"64MB\")\n",
    "    \n",
    "    # --- Feature Engineering and Preprocessing with Dask ---\n",
    "    print(\"Performing Dask feature engineering (One-Hot Encoding)...\")\n",
    "    # Dask's get_dummies requires the column to be of 'category' dtype first.\n",
    "    ddf = ddf.categorize(columns=['vehicle_type'])\n",
    "    ddf_processed = dd.get_dummies(ddf, columns=['vehicle_type'], drop_first=True)\n",
    "    \n",
    "    # --- Define Features (X) and Target (y) ---\n",
    "    # We drop the calculated_risk_score as the model should learn this relationship itself.\n",
    "    X = ddf_processed.drop(['annual_premium_quote', 'calculated_risk_score'], axis=1)\n",
    "    y = ddf_processed['annual_premium_quote']\n",
    "\n",
    "    # Persist the Dask collections in memory across the cluster to speed up subsequent computations\n",
    "    X = X.persist()\n",
    "    y = y.persist()\n",
    "\n",
    "    # Get the column layout for the inference script\n",
    "    # We compute the columns to get a concrete list from the Dask object.\n",
    "    model_columns = X.columns\n",
    "    \n",
    "    # --- Split Data ---\n",
    "    # Dask-ML provides its own train_test_split function.\n",
    "    # We add shuffle=True to address the FutureWarning and adopt the recommended behavior.\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=True)\n",
    "    print(f\"Data split into training and testing sets.\")\n",
    "\n",
    "    # --- Initialize and Train the Dask-XGBoost Model ---\n",
    "    print(\"\\nTraining Dask-XGBoost Regressor model for premium quotation...\")\n",
    "    # Use the DaskXGBRegressor class from the official xgboost.dask module\n",
    "    model = DaskXGBRegressor(\n",
    "        objective='reg:squarederror',\n",
    "        n_estimators=1000,\n",
    "        learning_rate=0.05,\n",
    "        max_depth=6,\n",
    "        subsample=0.7,\n",
    "        colsample_bytree=0.7,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # The .fit() method will now trigger the Dask computation graph.\n",
    "    model.fit(X_train, y_train)\n",
    "              \n",
    "    print(\"Model training complete.\")\n",
    "    \n",
    "    # --- Create a single bundle containing the standard Booster model and columns ---\n",
    "    # Extract the core, non-Dask Booster object for portability\n",
    "    booster = model.get_booster()\n",
    "    \n",
    "    model_bundle = {\n",
    "        'model': booster,\n",
    "        'columns': model_columns\n",
    "    }\n",
    "    \n",
    "    # --- Save the bundle to a single file ---\n",
    "    joblib.dump(model_bundle, model_bundle_output_file)\n",
    "    print(f\"\\nModel and column info bundled and saved to '{model_bundle_output_file}'.\")\n",
    "    \n",
    "    # --- Evaluate the Model ---\n",
    "    print(\"\\n--- Evaluating Model Performance on Test Set ---\")\n",
    "    # We need to compute the results since they are lazy Dask collections.\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_test_computed, y_pred_computed = client.compute([y_test, y_pred], sync=True)\n",
    "    \n",
    "    mae = mean_absolute_error(y_test_computed, y_pred_computed)\n",
    "    r2 = r2_score(y_test_computed, y_pred_computed)\n",
    "    \n",
    "    print(f\"R-squared (R²): {r2:.4f}\")\n",
    "    print(f\"Mean Absolute Error (MAE): ${mae:,.2f}\")\n",
    "    \n",
    "    client.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train_underwriting_model_dask()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a22ccc0-bb06-487d-b743-ff05fd3f487a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
